---
slug: vector-search-openai
title: Vector Search with Tigris and OpenAI
description: >
  In this post, we'll show you how to use the OpenAI Embeddings API to generate
  embeddings for your documents and then use Tigris to index these embeddings
  for fast and scalable vector search.
keywords: [openai, vector search, embeddings, search]
authors: [ot]
tags: [openai, vector search, search]
image: ./vector-search-with-openai.jpeg
---

import tigrisConfig from "@site/tigris.config.js";

# Fast and scalable vector search with Tigris and OpenAI

In this post, we'll show you how to use the OpenAI Embeddings API to generate
embeddings for your documents and then use Tigris to index these embeddings
for fast and scalable vector search.

This is a powerful combination that can be used for building semantic search
applications, recommendation engines, and more.

## What is vector search?

Vector search is a type of search that uses vector representations of documents
to find similar documents. Vector search is a powerful technique that can be
used to find similar documents, images, and videos. Vector search is also
useful for finding similar products, recommendations, and more.

<!-- truncate-->

## What is an embedding?

An embedding is a vector representation of a document. Embeddings are
generated by a machine learning model that is trained on a large corpus of
documents. The model learns the relationships between words and documents and
then generates a vector representation of the document. The vector
representation is a dense vector of floating-point numbers that can be used
to find similar documents.

For more information, check out the [OpenAI embeddings guide](https://platform.openai.com/docs/guides/embeddings).

## Vector search workflow with OpenAI and Tigris

The vector search workflow with OpenAI and Tigris works as follows:

1. Generate embeddings for your documents using the OpenAI Embeddings API.
2. Index the embeddings in Tigris.
3. Pass the search query through the OpenAI Embeddings API to generate an
   embedding.
4. Use the embedding to search for similar documents in Tigris.
5. Get back semantically similar documents, even if they don't contain the
   search query keywords.

![Vector Search with Tigris and OpenAI](vector-search-with-openai.jpeg)

Now, let's build a vector search application that implements this workflow.

## Building the vector search application

To follow along with this post, you'll need:

- An OpenAI account. You can sign up for an account [here](https://platform.openai.com/)
- A Tigris Cloud account. You can sign up for a free account <a href={tigrisConfig.signupUrl}>here</a>
- [Node.js 18.13.0](https://nodejs.org/) or newer

Once you have signed up for the Tigris Cloud account, create a new project called `vectorsearchapp`.
Next, make a note of the `clientId` and `clientSecret`, which you can get from the **Application Keys**
section of the project.

### Bootstrap the application

Tigris provides a tool called `create-tigris-app` that you can use to quickly bootstrap a new Node.js
application. We will use this tool to create an application with Tigris and OpenAI clients already
configured.

```bash
npx create-tigris-app@latest --project vectorsearchapp --example vector-search-openai
```

<details>
<summary>Command output</summary>

```bash
Need to install the following packages:
  create-tigris-app@1.0.0-beta.17
Ok to proceed? (y) y
✔ What is the clientId? … client_id_here
✔ What is the clientSecret? … *******************************************************
Creating a new app in /Users/ovaistariq/projects/vectorsearchapp.

Downloading files for example vector-search-openai. This might take a moment.


Initializing project with template: vector-search-openai

Using npm.

Installing dependencies:
- @tigrisdata/core: ^1.0.0-beta.43
- csv-parse: ^5.3.6
- dotenv: ^16.0.3
- express: 4.18.2
- openai: ^3.2.1
- reflect-metadata: ^0.1.13
- zod: ^3.19.1


added 410 packages, and audited 411 packages in 12s

111 packages are looking for funding
  run `npm fund` for details

found 0 vulnerabilities
Initialized a git repository.


Success! Created vectorsearchapp at /Users/ovaistariq/projects/vectorsearchapp

Inside that directory, you can run several commands:

  npm run dev
    Starts the development server.

  npm run build
    Builds the app for production.

  npm start
    Runs the built app in production mode.

We suggest that you begin by typing:

  cd vectorsearchapp
  npm run dev

```

</details>

The `create-tigris-app` tool will create a new directory called `vectorsearchapp`. Let's
change into this directory.

```bash
cd vectorsearchapp
```

We will need the OpenAI Organization ID and API key to authenticate with OpenAI.
Visit the [API Keys](https://platform.openai.com/account/api-keys) page to retrieve the API key
and the [Org Settings](https://platform.openai.com/account/org-settings) page to retrieve
the Organization ID.

Then update the `.env` file with the OpenAI Organization ID and API key.

```bash
OPENAI_ORG=your_org_id_here
OPENAI_API_KEY=your_api_key_here
```

Now we are all set.

Next, we will demonstrate how simple it is to store the embeddings in Tigris and then use
the Tigris client to search for similar documents. We will use a subset of reviews from the
[Amazon fine-food reviews](https://www.kaggle.com/snap/amazon-fine-food-reviews) dataset.

### Generating embeddings with OpenAI

Let's run the following command to generate embeddings for the reviews and store the
reviews and embeddings in Tigris.

```bash
npm run seed
```

The command performs the following steps:

#### 1. Creates a Tigris Search Index

The search index called `reviews` is defined with the following schema:

```ts title="src/search/models/review.ts"
import {
  SearchField,
  TigrisDataTypes,
  TigrisSearchIndex,
} from "@tigrisdata/core";

export const REVIEW_INDEX_NAME = "reviews";

@TigrisSearchIndex(REVIEW_INDEX_NAME)
export class Review {
  @SearchField(TigrisDataTypes.INT64)
  Id: string;

  @SearchField({ sort: true, facet: true })
  ProductId: string;

  @SearchField()
  UserId: string;

  @SearchField()
  ProfileName: string;

  @SearchField(TigrisDataTypes.INT64)
  HelpfulnessNumerator: number;

  @SearchField(TigrisDataTypes.INT64)
  HelpfulnessDenominator: number;

  @SearchField(TigrisDataTypes.INT64, { sort: true, facet: true })
  Score: number;

  @SearchField(TigrisDataTypes.INT64, { sort: true })
  Time: number;

  @SearchField()
  Summary: string;

  @SearchField()
  Text: string;

  // 1536 floats total for ada-002
  @SearchField({ dimensions: 1536 })
  vector: number[];
}
```

And the index is created with the following code:

```ts title="scripts/seed.ts"
// ...
import { Review, REVIEW_INDEX_NAME } from "../src/search/models/review";
import { Tigris } from "@tigrisdata/core";
// ...

const tigrisClient = new Tigris();
const search = tigrisClient.getSearch();
await search.createOrUpdateIndex(Review);
// ...
```

#### 2. Reads the reviews from the CSV file AND generates embeddings

The reviews are loaded from the CSV file and embeddings are generated via OpenAI:

```ts title="scripts/seed.ts"
// ...
const index = await search.getIndex<Review>(REVIEW_INDEX_NAME);

const dataFile = "./scripts/data/reviews.csv";
// ...
for await (const doc of readStream.pipe(parse({ columns: true, cast: true }))) {
  if (batch.length >= batchSize) {
    await insertBatch(batch, index);

    numDocsLoaded += batch.length;
    batch.length = 0;
  } else {
    const embeddings = await createEmbedding(
      openai,
      `${doc.Summary} ${doc.Text}`
    );
    doc.vector = embeddings;

    batch.push(doc as Review);
  }
}

if (batch.length > 0) {
  await insertBatch(batch, index);

  numDocsLoaded += batch.length;
  batch.length = 0;
}
// ...
```

The `createEmbedding` function uses the OpenAI client to generate embeddings for the review:

```ts title="src/utils/openai.ts"
import { Configuration, OpenAIApi } from "openai";

const MODEL = "text-embedding-ada-002";

const getOpenaiClient = (): OpenAIApi => {
  // setup OpenAI client
  const config = new Configuration({
    organization: process.env.OPENAI_ORG,
    apiKey: process.env.OPENAI_API_KEY,
  });
  return new OpenAIApi(config);
};

const createEmbedding = async (
  openai: OpenAIApi,
  input: string
): Promise<number[]> => {
  const embeddings = await openai.createEmbedding({
    model: MODEL,
    input: input,
  });
  return embeddings.data.data[0]?.embedding;
};
```

#### 3. Store reviews and embeddings in Tigris

The `insertBatch` function is called, which makes use of the Tigris `createOrReplaceMany` SDK function
to store the embeddings in the Tigris search index:

```ts title="scripts/seed.ts"
async function insertBatch(batch: Review[], index: SearchIndex<Review>) {
  const docStatus = await index.createOrReplaceMany(batch);
  for (const status of docStatus) {
    if (status.error) {
      console.log(status.error);
      console.log(batch);

      throw new Error("Error creating documents");
    }
  }
}
```

Now that the embeddings have been generated and stored in Tigris, we can start the application
and perform searches.

## Running the vector search application

Let's run the following command to start the application in development mode running on `http://localhost:3000`:

```bash
npm run dev
```

<details>
<summary>Command Output</summary>

```bash
> vectorsearchapp@0.1.0 predev
> npm run setup


> vectorsearchapp@0.1.0 setup
> npx ts-node scripts/setup.ts

info - Using reflection to infer type of Review#ProductId
info - Using reflection to infer type of Review#UserId
info - Using reflection to infer type of Review#ProfileName
info - Using reflection to infer type of Review#Summary
info - Using reflection to infer type of Review#Text
info - Using reflection to infer type of Review#vector
info - Using Tigris at: api.preview.tigrisdata.cloud:443
Created index:
Setup complete ...

> vectorsearchapp@0.1.0 dev
> npx nodemon

[nodemon] 2.0.22
[nodemon] to restart at any time, enter `rs`
[nodemon] watching path(s): src/**/*
[nodemon] watching extensions: ts
[nodemon] starting `npx ts-node ./src/index.ts`
info - Using reflection to infer type of Review#ProductId
info - Using reflection to infer type of Review#UserId
info - Using reflection to infer type of Review#ProfileName
info - Using reflection to infer type of Review#Summary
info - Using reflection to infer type of Review#Text
info - Using reflection to infer type of Review#vector
info - Using Tigris at: api.preview.tigrisdata.cloud:443

🚀 Server ready at: http://localhost:3000 ⭐️

```

</details>

### Searching for similar reviews

Let's perform some searches now.

```bash
curl \
  'http://localhost:3000/search?searchString=hot%20sauce&size=5' \
  -H 'Content-Type: application/json'
```

This returns the following results:

```json
[
  {
    "Id": 988,
    "ProductId": "B006F2NYI2",
    "UserId": "A36GDATSF85X7W",
    "ProfileName": "Matt Waz",
    "Score": 5,
    "Summary": "Awesome Sauce",
    "Text": "This hot sauce is one of my favorites. Its a perfect balance of tasty and spicy. You can smell the flavors immediately when you open the bottle. A mix of garlic, habanero, and other spices. I put this on everything and tend to go through one bottle a week. I highly recommend this to anyone who likes hot sauce.",
    "VectorDistance": 0.14292240142822266
  },
  {
    "Id": 995,
    "ProductId": "B006F2NYI2",
    "UserId": "A1T5CH6SHV989P",
    "ProfileName": "a biemold",
    "Score": 5,
    "Summary": "best hot sauce around",
    "Text": "absolutely love the habenaro sauce...use it on eggs, sandwiches most anything for a good kick! have watched the progessing popularity of theis homegrown product...have nothing but the highes accolades for the chef and the product..get out there and try it!",
    "VectorDistance": 0.14448332786560059
  },
  {
    "Id": 994,
    "ProductId": "B006F2NYI2",
    "UserId": "A1KFUVZ3BZ59R1",
    "ProfileName": "merplinger",
    "Score": 5,
    "Summary": "Best all around hot sauce",
    "Text": "So far I have had the habanero and the medium sauces and they were amazing. My fiance and I use them nearly daily and I just picked up 3 more. It goes really well on all sorts of dishes to add the extra kick.<br /><br />The sauce itself lasts a lot longer than it appears. It is thick and a bottle will last us a couple of weeks of heavy use. We love to use it for breakfast on eggs or in burritos and for dinners or soups for added spice.",
    "VectorDistance": 0.1463753581047058
  },
  {
    "Id": 998,
    "ProductId": "B006F2NYI2",
    "UserId": "A3G313KLWDG3PW",
    "ProfileName": "kefka82",
    "Score": 5,
    "Summary": "this sauce is the shiznit",
    "Text": "this sauce is so good with just about anything, i like adding it to asian food or anything with egg or noodles, it has a good burn with a strong flavor. im hoping to see some of the other flavors like the pineapple experiment on here soon. buy it you wont regret it",
    "VectorDistance": 0.1499418020248413
  },
  {
    "Id": 996,
    "ProductId": "B006F2NYI2",
    "UserId": "A1D3F6UI1RTXO0",
    "ProfileName": "Swopes",
    "Score": 5,
    "Summary": "Hot & Flavorful",
    "Text": "BLACK MARKET HOT SAUCE IS WONDERFUL.... My husband Loves this Habenero sauce, its very very flavorful with a nice kick to it. I in fact love the medium sauce- the taste is so yummy with a bit of zing behind it. We used it for everything and you can use it with any meal.<br /><br />I like the fact that there are so many different levels of spicyness. We love the medium & habenero. Kind of like a \"HIS & HERS package\". Thank you for your hard work guys! Keep up the hard work.",
    "VectorDistance": 0.15223878622055054
  }
]
```

We have searched for "hot sauce" and the results are sorted by the vector distance. The
vector distance is a measure of how similar the search string is to the review text. The
closer the vector distance is to 0, the more similar the review text is to the search
string.

Here is what the code for the vector search API looks like:

```ts title="src/routes/search.ts"
// ...
import { Search, SearchQuery } from "@tigrisdata/core";
import middlewares from "../utils/middlewares";
import { Review, REVIEW_INDEX_NAME } from "../search/models/review";
import { createEmbedding } from "../utils/openai";
// ...
async (req, res, next) => {
  const { searchString, page, size } = req.query;

  const index = await searchClient.getIndex<Review>(REVIEW_INDEX_NAME);
  const embeddings = await createEmbedding(openai, searchString as string);

  const request: SearchQuery<Review> = {
    vectorQuery: {
      vector: embeddings,
    },
    facets: ["ProductId", "Score"],
    hitsPerPage: Number(size) || 10,
  };

  index
    .search(request, Number(page) || 1)
    .then((results) => {
      const docs = [];
      for (const hit of results.hits) {
        docs.push({
          Id: hit.document.Id,
          ProductId: hit.document.ProductId,
          UserId: hit.document.UserId,
          ProfileName: hit.document.ProfileName,
          Score: hit.document.Score,
          Summary: hit.document.Summary,
          Text: hit.document.Text,
          VectorDistance: hit.meta.textMatch.vectorDistance,
        });
      }
      res.json(docs);
    })
    .catch((error) => next(error));
};
// ...
```

## Next Steps

Try out this application, and [let us know](https://www.tigrisdata.com/contact/) what
other examples of vector search you'd like to see.

If you have any questions or you'd like to contribute to the
[Tigris open source project](https://github.com/tigrisdata/tigris), it would be great to
see you in the [Tigris Discord Community](https://www.tigrisdata.com/discord/).
For anything else, [get in touch](https://www.tigrisdata.com/contact/).

---

import NewsletterSubscribe from "../../src/components/NewsletterSubscribe";

<NewsletterSubscribe />
