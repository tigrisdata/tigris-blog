---
slug: distributed-training-lancedb
title: "Distributed Training with LanceDB and Tigris"
description: >
  Learn how to use LanceDB on top of Tigris to manage massive training datasets
  and stream them into model training, all without having to manage any
  infrastructure yourself.
image: ./hacking-collaboration.webp
keywords:
  - "LanceDB"
  - "Tigris"
  - "Distributed Training"
  - "PyTorch"
  - "AI"
  - "ML"
authors: [xe, ks]
tags: [features, engineering, lancedb, pytorch]
---

import Tabs from "@theme/Tabs";
import TabItem from "@theme/TabItem";

import InlineCta from "@site/src/components/InlineCta";

![](./hacking-collaboration.webp)

Training AI models on real world data, especially data that expands quickly like
video streams or feeds of user created photos, makes it critical to stream data
to your training jobs. Data that large does not fit into ram. What options do
you have?

{/* truncate */}

| üèóÔ∏è **Approach**            | üíæ **Storage Format**       | üöÄ **Streaming**         | üîÑ **DDP Compatible**       | ‚öôÔ∏è **Setup Complexity** |
| -------------------------- | --------------------------- | ------------------------ | --------------------------- | ----------------------- |
| **Custom IterableDataset** | Raw files + manual sharding | ‚úÖ Manual implementation | ‚úÖ Manual sharding required | üî¥ High                 |
| **WebDataset**             | `.tar` shards over HTTP/S3  | ‚úÖ Built-in streaming    | ‚úÖ Automatic                | üü° Medium               |
| **HuggingFace Datasets**   | Various formats on HF Hub   | ‚úÖ Streaming mode        | ‚ö†Ô∏è Partial support          | üü¢ Low                  |
| **LanceDB + Tigris**       | Optimized columnar format   | ‚úÖ Native streaming      | ‚úÖ Built-in sharding        | üü¢ Low                  |

What if there were a better way to do this? What if you could store however much
data you wanted and let the infrastructure layer figure it all out for you? This
is possible with the combination of [LanceDB](https://www.lancedb.com/), a
Multimodal Lakehouse layer for your data, and Tigris, a globally performant
object storage backend that puts your files close to where they‚Äôre being used.
LanceDB uses the Lance columnar format (built on Apache Arrow) to store data
ranging from images, videos, text, or embedding vectors, all with features like
versioning and fast random access. LanceDB can also store raw bytes as columns,
so you don‚Äôt need to manage your data separately from your database.

In this guide, we‚Äôll show how you can use LanceDB on top of Tigris to manage
massive training datasets and stream them into model training, all without
having to manage any infrastructure yourself. We‚Äôll walk through practical steps
and code examples for:

- Storing training data such as the
  [Food101 Dataset](https://www.kaggle.com/datasets/dansbecker/food-101) in the
  Lance format and uploading it to Tigris.
- Authenticating and connecting LanceDB to Tigris using environment variables or
  hardcoded credentials.
- Loading and streaming data from Tigris into PyTorch using LanceDB‚Äôs
  `LanceDataset` (streaming) and `SafeLanceDataset` (in-memory) classes.
- Integrating LanceDB into your PyTorch `DataLoader` and typical training loops.
- How to pick between `LanceDataset` and `SafeLanceDataset`, so you know when to
  choose the best tool for the job.

By the end, you‚Äôll be able to scale your training workflows with a LanceDB +
Tigris stack. This lets you keep your datasets close to where they‚Äôre being used
in a multimodal lakehouse on the cloud without having to worry about local
storage, data placement, or egress fees. LanceDB and Tigris handle those hard
parts for you so you can get back to training and using your models.

### Prerequisites

To get started, you need the following:

- A **Python environment** with **PyTorch** and **TorchVision** (for the Food101
  dataset) installed. Install the Lance library via pip: `pip install lance`.
- An account with [Tigris](https://www.tigrisdata.com) from
  [storage.new](https://storage.new).
- A bucket created in that account (for example, `my-bucket`).
- An access keypair from [storage.new/accesskey](https://storage.new/accesskey).

## Setting up LanceDB with Tigris

To use LanceDB as our data lakehouse, we need to configure it to use Tigris as a
storage backend. Tigris
[works with LanceDB](https://www.tigrisdata.com/docs/libraries/lancedb/) because
Tigris exposes the S3 API, which LanceDB can use to read and write data. All you
need to do is change out the credentials and endpoints.

### Authentication via Environment Variables

The LanceDB client picks up credentials from environment variables. Set the
following environment variables with your Tigris credentials either in your
shell or in a `.env` file:

<Tabs>
<TabItem value="shell" label="Shell" default>

```shell
export AWS_ACCESS_KEY_ID=tid_access_key_id
export AWS_SECRET_ACCESS_KEY=tsec_secret_access_key
export AWS_ENDPOINT_URL_S3=https://t3.storage.dev
export AWS_REGION=auto
```

</TabItem>
<TabItem value="env-file" label=".env file">

```
AWS_ACCESS_KEY_ID=tid_access_key_id
AWS_SECRET_ACCESS_KEY=tsec_secret_access_key
AWS_ENDPOINT_URL_S3=https://t3.storage.dev
AWS_REGION=auto
```

</TabItem>
</Tabs>

Make sure `AWS_ACCESS_KEY_ID` and `AWS_SECRET_ACCESS_KEY` match the values you
get from the Tigris console. The other variables tell LanceDB where to look for
your data. Once these variables are exported into or loaded by your training
loop, LanceDB will use them internally to authenticate with Tigris.

### Authentication via Hardcoded Credentials

:::warning

Hardcoding access credentials is a bad idea from a security standpoint. These
credentials will allow an attacker to delete or modify your training dataset
bucket. Please only do this if you are unable to set the credentials in your
platform of choice properly.

:::

You can also pass credentials directly to LanceDB in your code. LanceDB‚Äôs
`connect` method accepts a `storage_options` dictionary to let you specify
whatever credentials you want. This is useful if you are in an environment where
you need to retrieve credentials from a secret store at runtime.

### Connecting to Tigris

Now, let‚Äôs connect LanceDB to your Tigris bucket. In Python, use the
[LanceDB SDK](https://lancedb.github.io/lancedb/python/python/) to connect to
your database in Tigris. For example, if your bucket is `my-bucket` and you want
to store a dataset under the path `ml-datasets/food101`, you can connect like
this:

```py
import lancedb

# Connect to LanceDB database in a Tigris bucket (s3-compatible URI for Tigris)
db = lancedb.connect(
    "s3://my-bucket/ml-datasets/food101",
    storage_options={
        "endpoint": "https://t3.storage.dev", # Tigris storage endpoint
        "region": "auto",                     # auto region for global routing
        # If you are not using env vars, you can also specify credentials here:
        # "aws_access_key_id": "tid_access_key_id",
        # "aws_secret_access_key": "tsec_secret_access_key",
    }
)
```

In this example, we connected to the path `s3://my-bucket/ml-datasets/food101`
with the Tigris endpoint and region we specified. The `db` object is a handle to
a LanceDB database stored remotely on Tigris. We‚Äôve configured it to talk to
Tigris instead of S3 by setting the endpoint to Tigris‚Äô
[`https://t3.storage.dev`](https://t3.storage.dev), so even though the path
starts with `s3://`, it‚Äôs actually talking to Tigris.

## Converting the Dataset to LanceDB and Storing it in Tigris

To convert the [Food101](https://www.kaggle.com/datasets/dansbecker/food-101)
dataset to LanceDB, we need to do the following:

1.  **Load the dataset**: Use TorchVision to download the Food101 images and
    labels.
2.  **Batch the data into an Arrow table:** Iterate through images, convert each
    to raw bytes (such as raw JPEG bytes), and accumulate them in lists. Once a
    batch is collected (e.g., 1024 images), convert the lists into a PyArrow
    table and yield it as a RecordBatch.
3.  **Write to Lance format:** Use `lance.write_dataset()` to write the
    RecordBatches to a Lance file. We specify a _fragment size_ (number of rows
    per fragment) to partition the dataset into multiple shards on disk (e.g.,
    8192 images per fragment) for optimized reading and parallelism.
4.  **Upload to Tigris**: Instead of writing to the local disk, we use an
    `s3://<bucket>/...` URI that points to our Tigris bucket. With the right
    credentials in environment variables, Lance will write to that Tigris bucket
    over the S3 API.

Here‚Äôs a code snippet that converts the Food 101 dataset into a LanceDB table
stored in Tigris:

<details>
<summary>Code to convert Food101 to LanceDB and store in Tigris</summary>

```py
import os, io
import pyarrow as pa
import lance
from torchvision.datasets import Food101
from tqdm import tqdm

# Helper to create a LanceDB dataset from an image classification dataset
def create_lance_dataset(dataset, output_uri, fragment_size=12500, batch_size=1024):
    # Define schema for the Lance dataset (image bytes and label)
    schema = pa.schema([
        ("image", pa.binary()),
        ("label", pa.int64())
    ])
    images, labels = [], []

    def batch_generator():
        """Yield pyarrow RecordBatches each of size up to `batch_size`, fragmented to `fragment_size`."""
        for idx, (img, label) in enumerate(tqdm(dataset, desc=f"Converting to Lance ({output_uri})")):
            # Convert image to bytes (JPEG format)
            buffer = io.BytesIO()
            img.save(buffer, format="JPEG")
            images.append(buffer.getvalue())
            labels.append(label)
            # Once batch_size images collected, yield as an Arrow RecordBatch
            if (idx + 1) % batch_size == 0:
                table = pa.Table.from_arrays(
                    [pa.array(images, type=pa.binary()),
                     pa.array(labels, type=pa.int64())],
                    names=["image", "label"]
                )
                # Yield table in fragments (shards) of up to fragment_size rows each
                for batch in table.to_batches(max_chunksize=fragment_size):
                    yield batch
                images.clear(); labels.clear()
        # Yield any remaining images as final batch
        if images:
            table = pa.Table.from_arrays(
                [pa.array(images, type=pa.binary()),
                 pa.array(labels, type=pa.int64())],
                names=["image", "label"]
            )
            for batch in table.to_batches(max_chunksize=fragment_size):
                yield batch

    # Write the dataset to Lance format at the specified URI (local or s3)
    lance.write_dataset(batch_generator(), schema=schema, uri=output_uri, mode="overwrite",
                        max_rows_per_file=fragment_size)
    print(f"‚úÖ Lance dataset written to {output_uri}")

# Load Food101 training and test data (will download if not already present)
train_data = Food101(root="data/food101", split="train", download=True)
test_data  = Food101(root="data/food101", split="test", download=True)

# Specify your Tigris bucket name for output URIs
bucket_name = "my-bucket"  # Replace with your Tigris bucket name
train_uri = f"s3://{bucket_name}/food101_train.lance"
test_uri  = f"s3://{bucket_name}/food101_test.lance"

# Create LanceDB datasets for train and test splits, stored in the Tigris bucket
create_lance_dataset(train_data, output_uri=train_uri)
create_lance_dataset(test_data,  output_uri=test_uri)
```

</details>

In this code:

- We use `Food101(root="data/food101", split="...")` to download the images and
  get a dataset object. Each item from `Food101` is a tuple
  `(PIL_image, label_index)`.
- The `create_lance_dataset` function iterates through the dataset with a
  progress bar (using **tqdm**). For each image, we convert it to bytes using
  PIL‚Äôs `save()` (writing to an in-memory `BytesIO` buffer as JPEG). We
  accumulate images and labels in Python lists.
- Every `batch_size` images (here 1024), we convert the lists to a PyArrow Table
  and then yield it in **RecordBatch** chunks of up to `fragment_size` rows.
  This two-level batching ensures we don‚Äôt hold all data in memory: at most
  `batch_size` images are in memory before writing out. The `fragment_size`
  (12,500) means the Lance dataset will be sharded into fragments of 12,500 rows
  (images) each, which improves read parallelism and avoids giant files.
- We call `lance.write_dataset()` with our `batch_generator()`, schema, and the
  S3 URI of the output. The `mode="overwrite"` will replace any existing data at
  that path. The Lance library handles streaming our record batches to the
  object store. With the S3 endpoint and credentials configured, the data is
  **uploaded to the Tigris bucket** in Lance format (you will see `.lance` files
  and fragment subdirectories appear in the bucket).

After running that code, we have two Lance datasets stored in Tigris: one for
training and one for testing. We can verify the datasets are in the bucket by
looking through the Tigris console or listing files with `aws s3 ls`. Each
LanceDB dataset is a folder full of metadata and data shards.

:::note

There are plenty of ways to do this dataset import process. You could write it
to local disk and then copy it over with [rclone](https://rclone.org/) or
[aws s3 sync](https://docs.aws.amazon.com/cli/latest/reference/s3/sync.html), or
use any number of different ways to copy data. Writing it directly to object
storage lets you skip that middleman.

:::

Now we can load the dataset to do model training. Next, we‚Äôll see how to load
and stream this LanceDB dataset from Tigris during training.

## Loading LanceDB data for Training with PyTorch

Once your dataset is in Lance format on Tigris, you can train your models
without ever fully downloading the dataset to your training machine. Lance
provides two PyTorch `Dataset` classes for reading data. These are both
different takes on the same basic pattern: loading data from storage into
memory, but the main difference is how they end up doing it:

- `LanceDataset` is an **iterable-style** or **streaming** dataset (akin to
  `torch.utils.data.IterableDataset`). It streams data sequentially and is great
  for data that is bigger than memory or unindexed data. It also has built-in
  sharding logic to work in distributed training scenarios (where you have more
  than one GPU in the mix) without needing PyTorch‚Äôs `DistributedSampler`.
- `SafeLanceDataset` is a **map-style** or **in-memory** dataset (akin to
  `torch.utils.data.Dataset`). It supports random access by index, which allows
  PyTorch to use multiple workers to load data in parallel. Most tutorials and
  production workflows start out with this approach, and it‚Äôs best for
  fixed-size datasets because it can yield better performance with standard
  `DataLoader` shuffling and sampling.

In short, use `SafeLanceDataset` for data that you know is smaller than memory
and will never change. If the dataset is continuously growing or is so large
that it can‚Äôt fit on a single machine, use `LanceDataset`.

We‚Äôre going to show off what it looks like to use both options. In either case,
the goal is to decode image bytes into tensors and apply any preprocessing
transformations we need for training. We‚Äôll use a simple transformation: resize
each image to 224 by 224 pixels and convert them into tensors. If your training
job needs normalization or data augmentation, do that here.

### Loading from a `LanceDataset` (Streaming Iterable Dataset)

`LanceDataset` treats the dataset as a stream of data. It reads batches of data
directly from the LanceDB store instead of relying on the PyTorch `DataLoader`
class. In our case, `LanceDataset` will stream from the Lance file we created on
Tigris and yield one batch at a time.

One thing to note: `LanceDataset` yields data in sequence. Any preprocessing
should be done _within_ the `LanceDataset`‚Äôs `to_tensor_fn` callback. In other
words, since we aren‚Äôt using the native `DataLoader` collation function to
transform raw data (as we would in the in-memory style of dataset), we have to
provide a function that takes the raw batch and converts it into PyTorch
tensors. This is the same rough idea as a collation function, but it happens
when the dataset is being iterated. If your LanceDB table contains multimodal
data (like our image bytes), you **must** decode them in `to_tensor_fn` because
the raw bytes can‚Äôt be directly turned into Torch tensors without conversion.

Let‚Äôs create a `LanceDataset` for our training data and include a decoding
function:

<details>
<summary>Loading data with `LanceDataset`</summary>

```py
import torch
from PIL import Image
from torchvision import transforms
from lance.torch.data import LanceDataset
from torch.utils.data import DataLoader

# Define transformation for images (resize to 224x224 and convert to tensor)
_transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    # transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # optional normalization
])

def decode_batch(record_batch):
    """Decode a pyarrow RecordBatch of images and labels into a batch of tensors."""
    images = []
    labels = []
    # Convert Arrow batch to list of Python dicts
    for item in record_batch.to_pylist():
        # Decode image bytes to PIL, then apply transform to get tensor
        img = Image.open(io.BytesIO(item["image"])).convert("RGB")
        img_tensor = _transform(img)
        images.append(img_tensor)
        labels.append(item["label"])
    # Stack images and labels into tensors
    return {
        "image": torch.stack(images),
        "label": torch.tensor(labels, dtype=torch.long)
    }

# Specify your Tigris bucket name for output URIs
bucket_name = "my-bucket"  # Replace with your Tigris bucket name
train_uri = f"s3://{bucket_name}/food101_train.lance"
test_uri  = f"s3://{bucket_name}/food101_test.lance"

# Initialize LanceDataset for the training set (reading directly from Tigris)
dataset = LanceDataset(train_uri, batch_size=128, to_tensor_fn=decode_batch)
# Use a DataLoader to iterate (num_workers=0 because LanceDataset handles batching internally)
loader = DataLoader(dataset, batch_size=None, num_workers=0)

# Fetch one batch to demonstrate
batch = next(iter(loader))
print("Batch images shape:", batch["image"].shape)  # e.g., [128, 3, 224, 224]
print("Batch labels shape:", batch["label"].shape)  # e.g., [128]
```

</details>

In this snippet, we open the data in the `LanceDataset` and use our
`decode_batch` function to process them fragment by fragment. We set
`num_workers=0` here to disable the native DataLoader parallelism because that
will stomp over what LanceDB does internally to make things safely parallel.
Internally, LanceDB uses a sharded batch sample to read contiguous chunks of
rows from each fragment across multiple workers or processes (keep reading for
multi-GPU training tips). The result is that each iteration of the DataLoader
returns a dictionary that‚Äôs ready to feed into the model.

:::note

In the code above, there‚Äôs a commented-out normalization transformation. If you
are using a pretrained model, uncomment that transformation.

:::

### Loading from a `SafeLanceDataset`

`SafeLanceDataset` is much closer to the usual PyTorch workflow: it allows the
use of multiple DataLoader workers and standard samplers. Each worker will
retrieve different samples by index. This is the typical approach for most use
cases (especially finite datasets like Food101) because it lets you use every
drop of CPU performance for the most speed possible.

The main difference between loading a `LanceDataset` and a `SafeLanceDataset` is
that our transformation logic happens with a `collate_fn` instead of a
`to_tensor_fn`. The collation function will do exactly what `decode_batch` did,
but it operates on a list of sample dicts instead of a RecordBatch.

We define `collate_fn(batch_of_dicts)` that takes a list of items (e.g.,
`{‚Äúimage‚Äù b‚Äù...‚Äù, ‚Äúlabel‚Äù: ...}` and performs the decoding and stacking:

```py
from lance.torch.data import SafeLanceDataset

def collate_fn(batch_list):
    """
    Collate a list of sample dicts from SafeLanceDataset into a batch dict.
    Decodes image bytes and applies transforms.
    """
    images = []
    labels = []
    for item in batch_list:
        # item is a dict like {"image": <bytes>, "label": <int>}
        img = Image.open(io.BytesIO(item["image"])).convert("RGB")
        img_tensor = _transform(img)
        images.append(img_tensor)
        labels.append(item["label"])
    return {
        "image": torch.stack(images),
        "label": torch.tensor(labels, dtype=torch.long)
    }

# Initialize SafeLanceDataset for training data
dataset_map = SafeLanceDataset(train_uri)  # uses the Lance dataset at the given URI
# Create DataLoader with multiple workers, using our collate_fn to handle decoding
loader_map = DataLoader(dataset_map, batch_size=128, shuffle=True, num_workers=8,
                        collate_fn=collate_fn, pin_memory=True)

# Fetch one batch to verify
batch = next(iter(loader_map))
print("Batch images shape:", batch["image"].shape)  # [128, 3, 224, 224]
print("Batch labels shape:", batch["label"].shape)   # [128]
```

Here we set `num_workers=8` (you can adjust based on your CPU cores and data
size; play with it to find the best number). Each worker process will load a
slice of the data concurrently. The `SafeLanceDataset` is safe to use with
multiple workers because each worker will open its own connection to the Lance
dataset and read different indices. When the DataLoader has collected 128
samples (from potentially across workers), it calls `collate_fn` to decode and
stack them into a batch. We enable `pin_memory=True` for faster host-to-GPU
transfer of the batch, since the DataLoader will return CPU tensors.

**Shuffle and Samplers:** We passed `shuffle=True` for randomness. You could
also use PyTorch‚Äôs `DistributedSampler` if training in distributed mode (see
next section), or Lance‚Äôs own fragment samplers (for Iterable datasets). In the
above, SafeLanceDataset will report a length equal to the number of rows
(images) in the Lance dataset, so DataLoader knows how many batches to expect
and can shuffle indices accordingly.

## What About Distributed Training?

If you use **Distributed Data Parallel (DDP)** for multi-GPU training with
`SafeLanceDataset`, you should use `DistributedSampler` or the `shuffle=True`
option (with the standard PyTorch DDP sharding behavior) to ensure each process
gets different data. In practice, you might initialize the DataLoader with
something like:

```py
sampler = DistributedSampler(train_dataset_map) if using_ddp else None
train_loader = DataLoader(train_dataset_map, batch_size=32, sampler=sampler, ... )
```

Just make sure to keep these tradeoffs in mind:

- With `LanceDataset` (iterable), Lance provides its own `ShardedBatchSampler`
  and `ShardedFragmentSampler` that are already aware of DDP. You can specify
  these when creating the `LanceDataset` (via the `sampler=` argument). For
  example, `LanceDataset(..., sampler=ShardedBatchSampler(...))` will ensure
  that every DDP process gets its own section of the batch. This gives you
  perfect load-balancing across GPUs by dividing batches for you. If you
  initialize the `LanceDataset` with no sampler in a DDP context, it will make
  every process read the entire dataset; this is exactly what you don‚Äôt want for
  training. It‚Äôs important to use shard-aware samplers in distributed mode with
  `LanceDataset`.
- With `SafeLanceDataset` (map-style), you can wrap the dataset with
  `torch.utils.data.distributed.DistributedSampler` so that each process / rank
  / GPU gets a distinct subset of the data per training epoch. PyTorch will
  ensure that each worker only reads its slice of the data. To use multiple
  GPUs, you could pass `sampler=DistributedSampler(dataset_map, ...)` to the
  DataLoader to use DDP. Remember to call `sampler.set_epoch(epoch)` each epoch
  when using distributed sampling so that shuffling is seeded differently per
  epoch.

Here‚Äôs what it looks like to use `LanceDataset` with DDP:

```py
# Assuming world_size and rank are given by the DDP environment
from lance.torch.data import ShardedBatchSampler
sampler = ShardedBatchSampler(rank=rank, world_size=world_size)
dataset = LanceDataset(train_uri, batch_size=128, to_tensor_fn=decode_batch, sampler=sampler)
loader = DataLoader(dataset, batch_size=None, num_workers=0)
```

This ensures each GPU gets its own portion of each batch, splitting the work up
equally.

In contrast, here‚Äôs what you‚Äôd do with `SafeLanceDataset`:

```py
from torch.utils.data.distributed import DistributedSampler
dataset_map = SafeLanceDataset(train_uri)
sampler = DistributedSampler(dataset_map, num_replicas=world_size, rank=rank, shuffle=True)
loader_map = DataLoader(dataset_map, batch_size=128, sampler=sampler, num_workers=8, collate_fn=collate_fn)
```

Either way, LanceDB on Tigris enables distributed training directly from cloud
storage without manually sharding the dataset or duplicating data per node. Each
worker will fetch exactly the data it needs from the shared Lance dataset in
Tigris thanks to these samplers.

## Conclusion

Using LanceDB as a multimodal lakehouse on top of Tigris lets ML engineers scale
their training pipelines with ease. Today we learned how to take a dataset that
lived on the local disk and move it into LanceDB (with all the benefits of
versioning, efficient I/O, and raw multimodal data storage) so you can leverage
Tigris‚Äô bottomless object storage and global availability. The LanceDB Python
integration via `LanceDataset` and `SafeLanceDataset` makes it simple to load
this remote data into your training loop with minimal changes. You can still use
your familiar PyTorch DataLoader patterns. The techniques we showed off today
can extend to other data types (just change out the `to_tensor_fn` or
`collate_fn` as facts and circumstances demand). This lets LanceDB act as your
unified solution for handling complex ML datasets.

By offloading data storage to Tigris, you eliminate the need to manage storage
infrastructure, worry about network filesystems, or any of the overhead
associated with either of those problems. Your training jobs can run anywhere
with a high-speed internet connection and pull multimodal data from the LanceDB
lakehouse as it‚Äôs needed. LanceDB ensures that the performance of your data
loading is optimized and that your data management (such as rolling back a bad
change) is straightforward and simple. When you combine LanceDB and Tigris, you
get an invincible power team that lets you treat data as data, no matter where
it happens to be located. This gives you fearless scaling from research to
development to production. Happy training!

<InlineCta
  title={"Scale Your ML Training with LanceDB + Tigris"}
  subtitle={
    "Stop managing storage infrastructure and start training models. Tigris provides globally distributed object storage so you don't have to worry about data placement and LanceDB gives you a multimodal lakehouse that streams data directly to your training jobs."
  }
  button={"Give me that!"}
/>
